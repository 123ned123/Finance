{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ecb9f5",
   "metadata": {},
   "source": [
    "## sqlite prac ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393a3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'test_database2.db' and table 'employees' created successfully.\n",
      "\n",
      "--- Schema View (Raw SQL) ---\n",
      "CREATE TABLE employees (\n",
      "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "            name TEXT NOT NULL,\n",
      "            role TEXT,\n",
      "            salary REAL,\n",
      "            hire_date TEXT\n",
      "        )\n",
      "\n",
      "--- Schema View (Column Details) ---\n",
      "ID    Name            Type       NotNull    PK   \n",
      "--------------------------------------------------\n",
      "0     id              INTEGER    0          1    \n",
      "1     name            TEXT       1          0    \n",
      "2     role            TEXT       0          0    \n",
      "3     salary          REAL       0          0    \n",
      "4     hire_date       TEXT       0          0    \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os \n",
    "\n",
    "db_name = \"test_database2.db\"\n",
    "\n",
    "test_db_folder = r\"..\\test_db\"\n",
    "\n",
    "db_dir = os.path.join(test_db_folder, db_name)\n",
    "\n",
    "def create_and_view_schema():\n",
    "    # 1. Connect (This creates the file if it doesn't exist)\n",
    "    conn = sqlite3.connect(db_dir)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 2. Create a Table (Defining the Schema)\n",
    "    # We create a dummy table called 'employees'\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS employees (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL,\n",
    "            role TEXT,\n",
    "            salary REAL,\n",
    "            hire_date TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(f\"Database '{db_name}' and table 'employees' created successfully.\\n\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Method A: Get the raw CREATE statement (The \"Code\" view)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"--- Schema View (Raw SQL) ---\")\n",
    "    cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='employees'\")\n",
    "    schema_raw = cursor.fetchone()\n",
    "    \n",
    "    if schema_raw:\n",
    "        print(schema_raw[0])\n",
    "    else:\n",
    "        print(\"Table not found.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Method B: Get the detailed column info (The \"Table\" view)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n--- Schema View (Column Details) ---\")\n",
    "    print(f\"{'ID':<5} {'Name':<15} {'Type':<10} {'NotNull':<10} {'PK':<5}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # PRAGMA table_info returns: (cid, name, type, notnull, dflt_value, pk)\n",
    "    cursor.execute(f\"PRAGMA table_info(employees)\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    for col in columns:\n",
    "        cid, name, dtype, notnull, dflt, pk = col\n",
    "        print(f\"{cid:<5} {name:<15} {dtype:<10} {notnull:<10} {pk:<5}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_view_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573fc2c",
   "metadata": {},
   "source": [
    "## this is the sample code about getting the SP500 ticker value ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96a04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching S&P 500 tickers from Wikipedia...\n",
      "Error fetching tickers: HTTP Error 403: Forbidden\n",
      "No tickers found. Exiting.\n",
      "Found 0 tickers.\n",
      "Data retrieval complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Retreives the current S&P 500 tickers from Wikipedia.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        df = tables[0]\n",
    "        # Wikipedia uses dots (BRK.B) but Yahoo uses dashes (BRK-B)\n",
    "        tickers = df['Symbol'].str.replace('.', '-', regex=False).tolist()\n",
    "        return tickers\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tickers: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_sqlite(data, db_name=\"sp500_data.db\", table_name=\"stock_history\"):\n",
    "    \"\"\"Saves a pandas DataFrame to a SQLite database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        # 'if_exists=\"append\"' allows adding data in chunks\n",
    "        data.to_sql(table_name, conn, if_exists=\"append\", index=True)\n",
    "        conn.close()\n",
    "        print(f\"Successfully saved {len(data)} rows to {db_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to database: {e}\")\n",
    "\n",
    "\n",
    "# 1. Setup\n",
    "start_date = \"2001-01-01\"\n",
    "end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "db_name = \"sp500_data.db\"\n",
    "\n",
    "# 2. Get Tickers\n",
    "print(\"Fetching S&P 500 tickers from Wikipedia...\")\n",
    "tickers = get_sp500_tickers()\n",
    "if not tickers:\n",
    "    print(\"No tickers found. Exiting.\")\n",
    "    exit(1)\n",
    "print(f\"Found {len(tickers)} tickers.\")\n",
    "\n",
    "# 3. Download and Store in Batches\n",
    "# Downloading 500+ tickers at once for 20+ years can cause timeouts or memory issues.\n",
    "# We will process them in batches of 20.\n",
    "batch_size = 20\n",
    "\n",
    "# Initialize database (optional: clear old data if you want a fresh start)\n",
    "# open(db_name, 'w').close() \n",
    "\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch = tickers[i:i + batch_size]\n",
    "    print(f\"Processing batch {i // batch_size + 1}/{(len(tickers) // batch_size) + 1} ({batch[0]} - {batch[-1]})...\")\n",
    "    \n",
    "    try:\n",
    "        # group_by='ticker' ensures the DataFrame is structured correctly for stacking\n",
    "        data = yf.download(batch, start=start_date, end=end_date, group_by='ticker', progress=False, threads=True)\n",
    "        \n",
    "        # yfinance returns a multi-level column DataFrame (Ticker -> OHLCV).\n",
    "        # We need to reshape it to be database-friendly (Date, Ticker, Open, Close, etc.)\n",
    "        \n",
    "        # Stack the ticker level to create a 'Ticker' column\n",
    "        data = data.stack(level=0, future_stack=True)\n",
    "        \n",
    "        # Reset index to make 'Date' and 'Ticker' regular columns\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Rename columns to be SQL friendly\n",
    "        data.columns.name = None # Remove the name of the columns index\n",
    "        data = data.rename(columns={\n",
    "            'Date': 'date', \n",
    "            'Ticker': 'ticker', \n",
    "            'Open': 'open', \n",
    "            'High': 'high', \n",
    "            'Low': 'low', \n",
    "            'Close': 'close', \n",
    "            'Adj Close': 'adj_close', \n",
    "            'Volume': 'volume'\n",
    "        })\n",
    "        \n",
    "        # Save this batch to DB\n",
    "        save_to_sqlite(data, db_name)\n",
    "        \n",
    "        # Sleep slightly to be polite to the API\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process batch starting with {batch[0]}: {e}\")\n",
    "\n",
    "print(\"Data retrieval complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for SPY from 2001-01-01 to today...\n",
      "Successfully saved 6280 rows for SPY to sp500_data.db.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "tickers = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" ] \n",
    "\n",
    "def get_ticker(tickers):\n",
    "    \"\"\"\n",
    "    Retrieves historical data specifically for the SPY ETF (SPDR S&P 500 ETF Trust)\n",
    "    and saves it to the database.\n",
    "    \"\"\"\n",
    "    ticker = \"SPY\"\n",
    "    start_date = \"2001-01-01\"\n",
    "    end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    db_name = \"sp500_data.db\"\n",
    "    \n",
    "    print(f\"Fetching data for {ticker} from {start_date} to today...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Download Data\n",
    "        # auto_adjust=False ensures we get both 'Close' and 'Adj Close' columns\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No data found for SPY.\")\n",
    "            return\n",
    "\n",
    "        # 2. Format Data\n",
    "        # Reset index to turn the 'Date' index into a standard column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Add a 'ticker' column explicitly so we know these rows belong to SPY\n",
    "        data['ticker'] = ticker\n",
    "\n",
    "        # Rename columns to be lowercase and SQL-friendly\n",
    "        data = data.rename(columns={\n",
    "            'Date': 'date', \n",
    "            'Open': 'open', \n",
    "            'High': 'high', \n",
    "            'Low': 'low', \n",
    "            'Close': 'close', \n",
    "            'Adj Close': 'adj_close', \n",
    "            'Volume': 'volume'\n",
    "        })\n",
    "\n",
    "        # Select and reorder columns to ensure they match the database schema\n",
    "        # Note: If yfinance returns multi-level columns, we flatten them or select specific ones\n",
    "        # This check handles occasional formatting differences in yfinance versions\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "        final_columns = ['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "        data = data[final_columns]\n",
    "\n",
    "        # 3. Save to Database\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        data.to_sql(\"stock_history\", conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"Successfully saved {len(data)} rows for {ticker} to {db_name}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {ticker}: {e}\")\n",
    "\n",
    "# --- Test the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    get_spy_ticker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86aafd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date ticker       open       high        low       close  \\\n",
      "0  2001-01-02 00:00:00    SPY  132.00000  132.15625  127.56250  128.812500   \n",
      "1  2001-01-03 00:00:00    SPY  128.31250  136.00000  127.65625  135.000000   \n",
      "2  2001-01-04 00:00:00    SPY  134.93750  135.46875  133.00000  133.546875   \n",
      "3  2001-01-05 00:00:00    SPY  133.46875  133.62500  129.18750  129.187500   \n",
      "4  2001-01-08 00:00:00    SPY  129.87500  130.18750  127.68750  130.187500   \n",
      "\n",
      "   adj_close    volume  \n",
      "0  81.997665   8737500  \n",
      "1  85.936356  19431600  \n",
      "2  85.011391   9219000  \n",
      "3  82.236374  12911400  \n",
      "4  82.872902   6625300  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"sp500_data.db\")\n",
    "\n",
    "# Example: Get all Apple data\n",
    "df = pd.read_sql(\"SELECT * FROM stock_history WHERE ticker = 'SPY'\", conn)\n",
    "print(df.head())\n",
    "\n",
    "# Example: Get all closing prices for a specific date\n",
    "df_date = pd.read_sql(\"SELECT ticker, close FROM stock_history WHERE date = '2023-01-04 00:00:00'\", conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce83185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-02 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.156250</td>\n",
       "      <td>127.562500</td>\n",
       "      <td>128.812500</td>\n",
       "      <td>81.997665</td>\n",
       "      <td>8737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-03 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>128.312500</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>127.656250</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>85.936356</td>\n",
       "      <td>19431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-04 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>134.937500</td>\n",
       "      <td>135.468750</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.546875</td>\n",
       "      <td>85.011391</td>\n",
       "      <td>9219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-05 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>133.468750</td>\n",
       "      <td>133.625000</td>\n",
       "      <td>129.187500</td>\n",
       "      <td>129.187500</td>\n",
       "      <td>82.236374</td>\n",
       "      <td>12911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-08 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>129.875000</td>\n",
       "      <td>130.187500</td>\n",
       "      <td>127.687500</td>\n",
       "      <td>130.187500</td>\n",
       "      <td>82.872902</td>\n",
       "      <td>6625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6275</th>\n",
       "      <td>2025-12-15 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>685.739990</td>\n",
       "      <td>685.760010</td>\n",
       "      <td>679.250000</td>\n",
       "      <td>680.729980</td>\n",
       "      <td>678.724426</td>\n",
       "      <td>90811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6276</th>\n",
       "      <td>2025-12-16 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>679.229980</td>\n",
       "      <td>681.080017</td>\n",
       "      <td>674.979980</td>\n",
       "      <td>678.869995</td>\n",
       "      <td>676.869934</td>\n",
       "      <td>122030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6277</th>\n",
       "      <td>2025-12-17 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>679.890015</td>\n",
       "      <td>680.440002</td>\n",
       "      <td>671.200012</td>\n",
       "      <td>671.400024</td>\n",
       "      <td>669.421936</td>\n",
       "      <td>110625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6278</th>\n",
       "      <td>2025-12-18 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>677.599976</td>\n",
       "      <td>680.739990</td>\n",
       "      <td>674.900024</td>\n",
       "      <td>676.469971</td>\n",
       "      <td>674.476929</td>\n",
       "      <td>108650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>2025-12-19 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>676.590027</td>\n",
       "      <td>681.090027</td>\n",
       "      <td>676.469971</td>\n",
       "      <td>680.590027</td>\n",
       "      <td>680.590027</td>\n",
       "      <td>103506100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6280 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date ticker        open        high         low  \\\n",
       "0     2001-01-02 00:00:00    SPY  132.000000  132.156250  127.562500   \n",
       "1     2001-01-03 00:00:00    SPY  128.312500  136.000000  127.656250   \n",
       "2     2001-01-04 00:00:00    SPY  134.937500  135.468750  133.000000   \n",
       "3     2001-01-05 00:00:00    SPY  133.468750  133.625000  129.187500   \n",
       "4     2001-01-08 00:00:00    SPY  129.875000  130.187500  127.687500   \n",
       "...                   ...    ...         ...         ...         ...   \n",
       "6275  2025-12-15 00:00:00    SPY  685.739990  685.760010  679.250000   \n",
       "6276  2025-12-16 00:00:00    SPY  679.229980  681.080017  674.979980   \n",
       "6277  2025-12-17 00:00:00    SPY  679.890015  680.440002  671.200012   \n",
       "6278  2025-12-18 00:00:00    SPY  677.599976  680.739990  674.900024   \n",
       "6279  2025-12-19 00:00:00    SPY  676.590027  681.090027  676.469971   \n",
       "\n",
       "           close   adj_close     volume  \n",
       "0     128.812500   81.997665    8737500  \n",
       "1     135.000000   85.936356   19431600  \n",
       "2     133.546875   85.011391    9219000  \n",
       "3     129.187500   82.236374   12911400  \n",
       "4     130.187500   82.872902    6625300  \n",
       "...          ...         ...        ...  \n",
       "6275  680.729980  678.724426   90811000  \n",
       "6276  678.869995  676.869934  122030600  \n",
       "6277  671.400024  669.421936  110625200  \n",
       "6278  676.469971  674.476929  108650100  \n",
       "6279  680.590027  680.590027  103506100  \n",
       "\n",
       "[6280 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ad00e",
   "metadata": {},
   "source": [
    "## get all the ticket and data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db25b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for SPY from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for SPY to SPY_data.db.\n",
      "Fetching data for NVDA from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for NVDA to NVDA_data.db.\n",
      "Fetching data for AAPL from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for AAPL to AAPL_data.db.\n",
      "Fetching data for MSFT from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for MSFT to MSFT_data.db.\n",
      "Fetching data for AMZN from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for AMZN to AMZN_data.db.\n",
      "Fetching data for GOOGL from 2001-01-01 to today...\n",
      "Successfully saved 5378 rows for GOOGL to GOOGL_data.db.\n",
      "Fetching data for META from 2001-01-01 to today...\n",
      "Successfully saved 3426 rows for META to META_data.db.\n",
      "Fetching data for TSLA from 2001-01-01 to today...\n",
      "Successfully saved 3903 rows for TSLA to TSLA_data.db.\n",
      "Fetching data for BRK-B from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for BRK-B to BRK-B_data.db.\n",
      "Fetching data for JPM from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for JPM to JPM_data.db.\n",
      "Fetching data for AVGO from 2001-01-01 to today...\n",
      "Successfully saved 4128 rows for AVGO to AVGO_data.db.\n",
      "Fetching data for INTC from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for INTC to INTC_data.db.\n",
      "Fetching data for WMT from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for WMT to WMT_data.db.\n",
      "Fetching data for UNH from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for UNH to UNH_data.db.\n",
      "Fetching data for V from 2001-01-01 to today...\n",
      "Successfully saved 4477 rows for V to V_data.db.\n",
      "Fetching data for PYPL from 2001-01-01 to today...\n",
      "Successfully saved 2641 rows for PYPL to PYPL_data.db.\n",
      "Fetching data for MA from 2001-01-01 to today...\n",
      "Successfully saved 4933 rows for MA to MA_data.db.\n",
      "Fetching data for HD from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for HD to HD_data.db.\n",
      "Fetching data for BAC from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for BAC to BAC_data.db.\n",
      "Fetching data for COST from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for COST to COST_data.db.\n",
      "Fetching data for DIS from 2001-01-01 to today...\n",
      "Successfully saved 6288 rows for DIS to DIS_data.db.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "tickers = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" ] \n",
    "\n",
    "def get_ticker(tickers):\n",
    "    \"\"\"\n",
    "    Retrieves historical data specifically for the SPY ETF (SPDR S&P 500 ETF Trust)\n",
    "    and saves it to the database.\n",
    "    \"\"\"\n",
    "    for ticker in tickers:\n",
    "        start_date = \"2001-01-01\"\n",
    "        end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "        db_name = f\"{ticker}_data.db\"\n",
    "        \n",
    "        print(f\"Fetching data for {ticker} from {start_date} to today...\")\n",
    "\n",
    "        try:\n",
    "            # 1. Download Data\n",
    "            # auto_adjust=False ensures we get both 'Close' and 'Adj Close' columns\n",
    "            data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)\n",
    "            \n",
    "            if data.empty:\n",
    "                print(\"No data found for SPY.\")\n",
    "                return\n",
    "\n",
    "            # 2. Format Data\n",
    "            # Reset index to turn the 'Date' index into a standard column\n",
    "            data.reset_index(inplace=True)\n",
    "            \n",
    "            # Add a 'ticker' column explicitly so we know these rows belong to SPY\n",
    "            data['ticker'] = ticker\n",
    "\n",
    "            # Rename columns to be lowercase and SQL-friendly\n",
    "            data = data.rename(columns={\n",
    "                'Date': 'date', \n",
    "                'Open': 'open', \n",
    "                'High': 'high', \n",
    "                'Low': 'low', \n",
    "                'Close': 'close', \n",
    "                'Adj Close': 'adj_close', \n",
    "                'Volume': 'volume'})\n",
    "\n",
    "            # Select and reorder columns to ensure they match the database schema\n",
    "            # Note: If yfinance returns multi-level columns, we flatten them or select specific ones\n",
    "            # This check handles occasional formatting differences in yfinance versions\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "            final_columns = ['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "            data = data[final_columns]\n",
    "\n",
    "            # 3. Save to Database\n",
    "            conn = sqlite3.connect(db_name)\n",
    "            data.to_sql(\"stock_history\", conn, if_exists=\"append\", index=False)\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"Successfully saved {len(data)} rows for {ticker} to {db_name}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "\n",
    "tickers = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" ] \n",
    "get_ticker(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50b4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database will be saved to: C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for SPY...\n",
      "Saved SPY to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for NVDA...\n",
      "Saved NVDA to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for AAPL...\n",
      "Saved AAPL to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for MSFT...\n",
      "Saved MSFT to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for AMZN...\n",
      "Saved AMZN to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for GOOGL...\n",
      "Saved GOOGL to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for META...\n",
      "Saved META to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for TSLA...\n",
      "Saved TSLA to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for BRK-B...\n",
      "Saved BRK-B to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for JPM...\n",
      "Saved JPM to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for AVGO...\n",
      "Saved AVGO to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for INTC...\n",
      "Saved INTC to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for WMT...\n",
      "Saved WMT to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for UNH...\n",
      "Saved UNH to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for V...\n",
      "Saved V to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for PYPL...\n",
      "Saved PYPL to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for MA...\n",
      "Saved MA to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for HD...\n",
      "Saved HD to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for BAC...\n",
      "Saved BAC to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for COST...\n",
      "Saved COST to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for DIS...\n",
      "Saved DIS to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for TSM...\n",
      "Saved TSM to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for ARM...\n",
      "Saved ARM to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n",
      "Fetching data for BABA...\n",
      "Saved BABA to C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\\market_data_master.db\n"
     ]
    }
   ],
   "source": [
    "# ******** this is the production version to get the close and open prices and save to a specific folder ********\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "import os  # <--- Import this to handle paths\n",
    "\n",
    "tickers = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\", \"TSM\", \\\n",
    "          \"ARM\", \"BABA\"] \n",
    "\n",
    "def get_ticker(tickers):\n",
    "    \n",
    "    # --- CONFIGURATION ---\n",
    "    # 1. Define the specific folder path\n",
    "    # Windows Example: r\"C:\\Users\\YourName\\Documents\\FinanceData\"\n",
    "    # Mac/Linux Example: \"/Users/yourname/Documents/FinanceData\"\n",
    "    # The 'r' before the string handles backslashes safely in Windows\n",
    "    target_folder = r\"C:\\Users\\admin\\Desktop\\R\\Projects\\Stock\\stock_daily_data\"\n",
    "    \n",
    "    # 2. Define the file name\n",
    "    db_filename = \"market_data_master.db\"\n",
    "    \n",
    "    # 3. Create the full path safely\n",
    "    db_path = os.path.join(target_folder, db_filename)\n",
    "\n",
    "    # 4. Check if directory exists, if not, create it automatically\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "        print(f\"Created directory: {target_folder}\")\n",
    "\n",
    "    print(f\"Database will be saved to: {db_path}\")\n",
    "\n",
    "    # --- MAIN LOOP ---\n",
    "    start_date = \"2001-01-01\"\n",
    "    end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching data for {ticker}...\")\n",
    "\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)\n",
    "            \n",
    "            if data.empty:\n",
    "                continue\n",
    "\n",
    "            data.reset_index(inplace=True)\n",
    "            data['ticker'] = ticker\n",
    "\n",
    "            data = data.rename(columns={\n",
    "                'Date': 'date', 'Open': 'open', 'High': 'high', \n",
    "                'Low': 'low', 'Close': 'close', 'Adj Close': 'adj_close', \n",
    "                'Volume': 'volume'\n",
    "            })\n",
    "\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "            final_columns = ['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "            data = data[final_columns]\n",
    "\n",
    "            # --- CONNECT USING THE FULL PATH ---\n",
    "            conn = sqlite3.connect(db_path)  # <--- Use db_path here\n",
    "            \n",
    "            table_name = ticker.replace(\"-\", \"_\")\n",
    "            data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"Saved {ticker} to {db_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_ticker(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPY] Fetching Historical Prices...\n",
      "[SPY] Saved 6288 days of price history.\n",
      "[SPY] Fetching Fundamentals...\n",
      "[SPY] Saved fundamental data.\n"
     ]
    }
   ],
   "source": [
    "# test version for fundamentals and history\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# 1. The Mapping you created (API Keys -> Your Database Columns)\n",
    "FUNDAMENTAL_MAPPING = {\n",
    "    'last_price': 'last',\n",
    "    'bid': 'bid',\n",
    "    'ask': 'ask',\n",
    "    'bidSize': 'bid_size',\n",
    "    'askSize': 'ask_size',\n",
    "    'currentPrice': 'current_price',\n",
    "    'targetHighPrice': 'target_high_price',\n",
    "    'targetLowPrice': 'target_low_price',\n",
    "    'targetMeanPrice': 'target_mean_price',\n",
    "    'targetMedianPrice': 'target_median_price',\n",
    "    'recommendationKey': 'recommendation_key',\n",
    "    'numberOfAnalystOpinions': 'number_of_analyst_opinions',\n",
    "    'lastDividendValue': 'last_dividend_value',\n",
    "    'lastDividendDate': 'last_dividend_date',\n",
    "    'totalCash': 'total_cash',\n",
    "    'totalCashPerShare': 'total_cash_per_share',\n",
    "    'ebitda': 'ebitda',\n",
    "    'totalDebt': 'total_debt',\n",
    "    'quickRatio': 'quick_ratio',\n",
    "    'currentRatio': 'current_ratio',\n",
    "    'totalRevenue': 'total_revenue',\n",
    "    'debtToEquity': 'debt_to_equity',\n",
    "    'revenuePerShare': 'revenue_per_share',\n",
    "    'returnOnAssets': 'return_on_assets',\n",
    "    'returnOnEquity': 'return_on_equity',\n",
    "    'grossProfits': 'gross_profits',\n",
    "    'freeCashflow': 'free_cashflow',\n",
    "    'operatingCashflow': 'operating_cashflow',\n",
    "    'earningsGrowth': 'earnings_growth',\n",
    "    'revenueGrowth': 'revenue_growth',\n",
    "    'grossMargins': 'gross_margins',\n",
    "    'ebitdaMargins': 'ebitda_margins',\n",
    "    'operatingMargins': 'operating_margins',\n",
    "    'regularMarketChange': 'regular_market_change',\n",
    "    'regularMarketChangePercent': 'regular_market_change_percent',\n",
    "    'regularMarketPrice': 'regular_market_price',\n",
    "    'postMarketChangePercent': 'post_market_change_percent',\n",
    "    'postMarketPrice': 'post_market_price',\n",
    "    'postMarketChange': 'post_market_change',\n",
    "    'averageDailyVolume3Month': 'average_daily_volume_3_month',\n",
    "    'open': 'open_price',\n",
    "    'dayLow': 'day_low',\n",
    "    'dayHigh': 'day_high',\n",
    "    'regularMarketPreviousClose': 'regular_market_previous_close',\n",
    "    'regularMarketOpen': 'regular_market_open',\n",
    "    'regularMarketDayLow': 'regular_market_day_low',\n",
    "    'regularMarketDayHigh': 'regular_market_day_high',\n",
    "    'dividendRate': 'dividend_rate',\n",
    "    'dividendYield': 'dividend_yield',\n",
    "    'trailingPE': 'trail_PE',\n",
    "    'forwardPE': 'forward_PE',\n",
    "    'trailingEps': 'trail_Eps',\n",
    "    'forwardEps': 'forward_Eps',\n",
    "    'previous_close': 'prev_close',\n",
    "    'volume': 'volume',\n",
    "    'regularMarketVolume': 'reg_market_volume',\n",
    "    'averageVolume': 'avg_volume',\n",
    "    'averageVolume10days': 'ave_10d_volume',\n",
    "    'currency': 'currency',\n",
    "    'exchange': 'exchange',\n",
    "    'profitMargins': 'profit_margin',\n",
    "    'floatShares': 'float_shares',\n",
    "    'enterpriseValue': 'company_value',\n",
    "    'sharesOutstanding': 'outstanding_share',\n",
    "    'sharesShort': 'share_short',\n",
    "    'sharesPercentSharesOut': 'share_percent_shares_out',\n",
    "    'heldPercentInsiders': 'insider_held_percent',\n",
    "    'heldPercentInstitutions': 'institution_held_percent',\n",
    "    'shortRatio': 'short_ratio',\n",
    "    'shortPercentOfFloat': 'short_percent_of_float',\n",
    "    'impliedSharesOutstanding': 'implied_shares_outstanding',\n",
    "    'bookValue': 'book_value',\n",
    "    'priceToBook': 'price_2_book'\n",
    "}\n",
    "\n",
    "def save_historical_prices(ticker, db_name=\"sp500_data.db\"):\n",
    "    \"\"\"Downloads OHLCV history (2001-Today) and saves to 'stock_history' table.\"\"\"\n",
    "    start_date = \"2001-01-01\"\n",
    "    print(f\"[{ticker}] Fetching Historical Prices...\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch history\n",
    "        df = yf.download(ticker, start=start_date, progress=False, auto_adjust=False)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"[{ticker}] No price data found.\")\n",
    "            return\n",
    "\n",
    "        # Formatting\n",
    "        df.reset_index(inplace=True)\n",
    "        df['ticker'] = ticker\n",
    "        \n",
    "        # Rename standard columns to lower case\n",
    "        df.rename(columns={\n",
    "            'Date': 'date', 'Open': 'open', 'High': 'high', \n",
    "            'Low': 'low', 'Close': 'close', 'Adj Close': 'adj_close', \n",
    "            'Volume': 'volume'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Handle MultiIndex if present (common issue with new yfinance versions)\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "        # Keep only valid columns\n",
    "        valid_cols = ['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "        # Filter for columns that actually exist in the dataframe\n",
    "        cols_to_save = [c for c in valid_cols if c in df.columns]\n",
    "        df = df[cols_to_save]\n",
    "\n",
    "        # Save\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        df.to_sql(\"stock_history\", conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "        print(f\"[{ticker}] Saved {len(df)} days of price history.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error saving history: {e}\")\n",
    "\n",
    "def save_fundamentals(ticker, db_name=\"sp500_data.db\"):\n",
    "    \"\"\"Fetches CURRENT fundamental snapshot and saves to 'stock_fundamentals' table.\"\"\"\n",
    "    print(f\"[{ticker}] Fetching Fundamentals...\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch info dictionary\n",
    "        ticker_obj = yf.Ticker(ticker)\n",
    "        info = ticker_obj.info\n",
    "        \n",
    "        # Extract data using your MAPPING\n",
    "        data_row = {'ticker': ticker, 'date_scraped': datetime.date.today()}\n",
    "        \n",
    "        for api_key, my_col_name in FUNDAMENTAL_MAPPING.items():\n",
    "            # info.get(key, None) safely returns None if data is missing\n",
    "            data_row[my_col_name] = info.get(api_key, None)\n",
    "\n",
    "        # Convert single row dictionary to DataFrame\n",
    "        df = pd.DataFrame([data_row])\n",
    "        \n",
    "        # Save\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        df.to_sql(\"stock_fundamentals\", conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "        print(f\"[{ticker}] Saved fundamental data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error saving fundamentals: {e}\")\n",
    "\n",
    "def process_stock(ticker):\n",
    "    # 1. Save History (The 2001-Today Chart Data)\n",
    "    save_historical_prices(ticker)\n",
    "    \n",
    "    # 2. Save Fundamentals (The specific list of columns you asked for)\n",
    "    save_fundamentals(ticker)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change this to \"SPY\", \"NVDA\", or run a loop for all S&P 500\n",
    "    process_stock(\"SPY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8731c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPY] Fetching fundamental data...\n",
      "[SPY] Successfully saved fundamentals.\n",
      "[NVDA] Fetching fundamental data...\n",
      "[NVDA] Successfully saved fundamentals.\n",
      "[AAPL] Fetching fundamental data...\n",
      "[AAPL] Successfully saved fundamentals.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# 1. Define the Mapping (API Keys -> Your Column Names)\n",
    "FUNDAMENTAL_MAPPING = {\n",
    "    'last_price': 'last',\n",
    "    'bid': 'bid',\n",
    "    'ask': 'ask',\n",
    "    'bidSize': 'bid_size',\n",
    "    'askSize': 'ask_size',\n",
    "    'currentPrice': 'current_price',\n",
    "    'targetHighPrice': 'target_high_price',\n",
    "    'targetLowPrice': 'target_low_price',\n",
    "    'targetMeanPrice': 'target_mean_price',\n",
    "    'targetMedianPrice': 'target_median_price',\n",
    "    'recommendationKey': 'recommendation_key',\n",
    "    'numberOfAnalystOpinions': 'number_of_analyst_opinions',\n",
    "    'lastDividendValue': 'last_dividend_value',\n",
    "    'lastDividendDate': 'last_dividend_date',\n",
    "    'totalCash': 'total_cash',\n",
    "    'totalCashPerShare': 'total_cash_per_share',\n",
    "    'ebitda': 'ebitda',\n",
    "    'totalDebt': 'total_debt',\n",
    "    'quickRatio': 'quick_ratio',\n",
    "    'currentRatio': 'current_ratio',\n",
    "    'totalRevenue': 'total_revenue',\n",
    "    'debtToEquity': 'debt_to_equity',\n",
    "    'revenuePerShare': 'revenue_per_share',\n",
    "    'returnOnAssets': 'return_on_assets',\n",
    "    'returnOnEquity': 'return_on_equity',\n",
    "    'grossProfits': 'gross_profits',\n",
    "    'freeCashflow': 'free_cashflow',\n",
    "    'operatingCashflow': 'operating_cashflow',\n",
    "    'earningsGrowth': 'earnings_growth',\n",
    "    'revenueGrowth': 'revenue_growth',\n",
    "    'grossMargins': 'gross_margins',\n",
    "    'ebitdaMargins': 'ebitda_margins',\n",
    "    'operatingMargins': 'operating_margins',\n",
    "    'regularMarketChange': 'regular_market_change',\n",
    "    'regularMarketChangePercent': 'regular_market_change_percent',\n",
    "    'regularMarketPrice': 'regular_market_price',\n",
    "    'postMarketChangePercent': 'post_market_change_percent',\n",
    "    'postMarketPrice': 'post_market_price',\n",
    "    'postMarketChange': 'post_market_change',\n",
    "    'averageDailyVolume3Month': 'average_daily_volume_3_month',\n",
    "    'open': 'open_price',\n",
    "    'dayLow': 'day_low',\n",
    "    'dayHigh': 'day_high',\n",
    "    'regularMarketPreviousClose': 'regular_market_previous_close',\n",
    "    'regularMarketOpen': 'regular_market_open',\n",
    "    'regularMarketDayLow': 'regular_market_day_low',\n",
    "    'regularMarketDayHigh': 'regular_market_day_high',\n",
    "    'dividendRate': 'dividend_rate',\n",
    "    'dividendYield': 'dividend_yield',\n",
    "    'trailingPE': 'trail_PE',\n",
    "    'forwardPE': 'forward_PE',\n",
    "    'trailingEps': 'trail_Eps',\n",
    "    'forwardEps': 'forward_Eps',\n",
    "    'previous_close': 'prev_close',\n",
    "    'volume': 'volume',\n",
    "    'regularMarketVolume': 'reg_market_volume',\n",
    "    'averageVolume': 'avg_volume',\n",
    "    'averageVolume10days': 'ave_10d_volume',\n",
    "    'currency': 'currency',\n",
    "    'exchange': 'exchange',\n",
    "    'profitMargins': 'profit_margin',\n",
    "    'floatShares': 'float_shares',\n",
    "    'enterpriseValue': 'company_value',\n",
    "    'sharesOutstanding': 'outstanding_share',\n",
    "    'sharesShort': 'share_short',\n",
    "    'sharesPercentSharesOut': 'share_percent_shares_out',\n",
    "    'heldPercentInsiders': 'insider_held_percent',\n",
    "    'heldPercentInstitutions': 'institution_held_percent',\n",
    "    'shortRatio': 'short_ratio',\n",
    "    'shortPercentOfFloat': 'short_percent_of_float',\n",
    "    'impliedSharesOutstanding': 'implied_shares_outstanding',\n",
    "    'bookValue': 'book_value',\n",
    "    'priceToBook': 'price_2_book'\n",
    "}\n",
    "\n",
    "def get_fundamental_data(ticker, db_name=\"sp500_data.db\"):\n",
    "    \"\"\"\n",
    "    Fetches the current fundamental snapshot for a ticker and saves it to the database.\n",
    "    \"\"\"\n",
    "    print(f\"[{ticker}] Fetching fundamental data...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Fetch Data\n",
    "        stock = yf.Ticker(ticker)\n",
    "        # .info is where the fundamental dictionary lives\n",
    "        info_data = stock.info\n",
    "        \n",
    "        # 2. Extract Data using Mapping\n",
    "        # We start with ticker and date since fundamentals are a snapshot in time\n",
    "        row_data = {\n",
    "            'ticker': ticker,\n",
    "            'time_scraped': datetime.datetime.now()\n",
    "        }\n",
    "\n",
    "        for api_key, db_column in FUNDAMENTAL_MAPPING.items():\n",
    "            # .get() prevents crashing if a specific key is missing from Yahoo\n",
    "            row_data[db_column] = info_data.get(api_key, None)\n",
    "            \n",
    "            # Fallback logic for keys that might have alternate names (like EPS)\n",
    "            if db_column == 'trail_Eps' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('epsTrailing', None)\n",
    "            if db_column == 'forward_Eps' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('epsForward', None)\n",
    "            if db_column == 'volume' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('last_volume', None)\n",
    "\n",
    "        # 3. Create DataFrame\n",
    "        df = pd.DataFrame([row_data])\n",
    "\n",
    "        # 4. Save to Database\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        # We use a DIFFERENT table name 'stock_fundamentals' to avoid mixing with daily price history\n",
    "        df.to_sql(\"stock_fundamentals\", conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"[{ticker}] Successfully saved fundamentals.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Get fundamentals for SPY and NVDA\n",
    "    tickers_to_fetch = [\"SPY\", \"NVDA\", \"AAPL\"]\n",
    "    \n",
    "    for symbol in tickers_to_fetch:\n",
    "        get_fundamental_data(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11091d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the scheduled version \n",
    "# **** this is production version at the mini server to get fundamentals daily ****\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_NAME = \"market_data.db\"\n",
    "TICKERS = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" ] \n",
    "MARKET_OPEN = datetime.time(9, 30)\n",
    "MARKET_CLOSE = datetime.time(16, 0)\n",
    "\n",
    "# 1. Define the Mapping (kept exactly as you had it)\n",
    "FUNDAMENTAL_MAPPING = {\n",
    "    'last_price': 'last',\n",
    "    'bid': 'bid',\n",
    "    'ask': 'ask',\n",
    "    'bidSize': 'bid_size',\n",
    "    'askSize': 'ask_size',\n",
    "    'currentPrice': 'current_price',\n",
    "    'targetHighPrice': 'target_high_price',\n",
    "    'targetLowPrice': 'target_low_price',\n",
    "    'targetMeanPrice': 'target_mean_price',\n",
    "    'targetMedianPrice': 'target_median_price',\n",
    "    'recommendationKey': 'recommendation_key',\n",
    "    'numberOfAnalystOpinions': 'number_of_analyst_opinions',\n",
    "    'lastDividendValue': 'last_dividend_value',\n",
    "    'lastDividendDate': 'last_dividend_date',\n",
    "    'totalCash': 'total_cash',\n",
    "    'totalCashPerShare': 'total_cash_per_share',\n",
    "    'ebitda': 'ebitda',\n",
    "    'totalDebt': 'total_debt',\n",
    "    'quickRatio': 'quick_ratio',\n",
    "    'currentRatio': 'current_ratio',\n",
    "    'totalRevenue': 'total_revenue',\n",
    "    'debtToEquity': 'debt_to_equity',\n",
    "    'revenuePerShare': 'revenue_per_share',\n",
    "    'returnOnAssets': 'return_on_assets',\n",
    "    'returnOnEquity': 'return_on_equity',\n",
    "    'grossProfits': 'gross_profits',\n",
    "    'freeCashflow': 'free_cashflow',\n",
    "    'operatingCashflow': 'operating_cashflow',\n",
    "    'earningsGrowth': 'earnings_growth',\n",
    "    'revenueGrowth': 'revenue_growth',\n",
    "    'grossMargins': 'gross_margins',\n",
    "    'ebitdaMargins': 'ebitda_margins',\n",
    "    'operatingMargins': 'operating_margins',\n",
    "    'regularMarketChange': 'regular_market_change',\n",
    "    'regularMarketChangePercent': 'regular_market_change_percent',\n",
    "    'regularMarketPrice': 'regular_market_price',\n",
    "    'postMarketChangePercent': 'post_market_change_percent',\n",
    "    'postMarketPrice': 'post_market_price',\n",
    "    'postMarketChange': 'post_market_change',\n",
    "    'averageDailyVolume3Month': 'average_daily_volume_3_month',\n",
    "    'open': 'open_price',\n",
    "    'dayLow': 'day_low',\n",
    "    'dayHigh': 'day_high',\n",
    "    'regularMarketPreviousClose': 'regular_market_previous_close',\n",
    "    'regularMarketOpen': 'regular_market_open',\n",
    "    'regularMarketDayLow': 'regular_market_day_low',\n",
    "    'regularMarketDayHigh': 'regular_market_day_high',\n",
    "    'dividendRate': 'dividend_rate',\n",
    "    'dividendYield': 'dividend_yield',\n",
    "    'trailingPE': 'trail_PE',\n",
    "    'forwardPE': 'forward_PE',\n",
    "    'trailingEps': 'trail_Eps',\n",
    "    'forwardEps': 'forward_Eps',\n",
    "    'previous_close': 'prev_close',\n",
    "    'volume': 'volume',\n",
    "    'regularMarketVolume': 'reg_market_volume',\n",
    "    'averageVolume': 'avg_volume',\n",
    "    'averageVolume10days': 'ave_10d_volume',\n",
    "    'currency': 'currency',\n",
    "    'exchange': 'exchange',\n",
    "    'profitMargins': 'profit_margin',\n",
    "    'floatShares': 'float_shares',\n",
    "    'enterpriseValue': 'company_value',\n",
    "    'sharesOutstanding': 'outstanding_share',\n",
    "    'sharesShort': 'share_short',\n",
    "    'sharesPercentSharesOut': 'share_percent_shares_out',\n",
    "    'heldPercentInsiders': 'insider_held_percent',\n",
    "    'heldPercentInstitutions': 'institution_held_percent',\n",
    "    'shortRatio': 'short_ratio',\n",
    "    'shortPercentOfFloat': 'short_percent_of_float',\n",
    "    'impliedSharesOutstanding': 'implied_shares_outstanding',\n",
    "    'bookValue': 'book_value',\n",
    "    'priceToBook': 'price_2_book'\n",
    "}\n",
    "\n",
    "def get_fundamental_data(ticker):\n",
    "    \"\"\"\n",
    "    Fetches the current fundamental snapshot for a ticker and saves it to the database.\n",
    "    \"\"\"\n",
    "    print(f\"[{ticker}] Fetching fundamental data...\")\n",
    "\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info_data = stock.info\n",
    "        \n",
    "        row_data = {\n",
    "            'ticker': ticker,\n",
    "            'time_scraped': datetime.datetime.now()\n",
    "        }\n",
    "\n",
    "        for api_key, db_column in FUNDAMENTAL_MAPPING.items():\n",
    "            row_data[db_column] = info_data.get(api_key, None)\n",
    "            \n",
    "            # Fallback logic\n",
    "            if db_column == 'trail_Eps' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('epsTrailing', None)\n",
    "            if db_column == 'forward_Eps' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('epsForward', None)\n",
    "            if db_column == 'volume' and row_data[db_column] is None:\n",
    "                row_data[db_column] = info_data.get('last_volume', None)\n",
    "\n",
    "        df = pd.DataFrame([row_data])\n",
    "\n",
    "        conn = sqlite3.connect(DB_NAME)\n",
    "        # Using 'append' so every 20 mins adds a new snapshot (Time Series data)\n",
    "        df.to_sql(\"stock_fundamentals\", conn, if_exists=\"append\", index=False)\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"[{ticker}] Successfully saved fundamentals.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "\n",
    "def job():\n",
    "    \"\"\"\n",
    "    This function checks if the current time is within market hours.\n",
    "    If yes, it runs the scraper.\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now().time()\n",
    "    # Check if within 9:30 AM to 4:00 PM\n",
    "    if MARKET_OPEN <= now <= MARKET_CLOSE:\n",
    "        print(f\"\\n--- Starting Scheduled Run: {datetime.datetime.now()} ---\")\n",
    "        for symbol in TICKERS:\n",
    "            get_fundamental_data(symbol)\n",
    "        print(\"--- Run Complete ---\\n\")\n",
    "    else:\n",
    "        print(f\"Market Closed. Current time: {now}. Waiting...\", end='\\r')\n",
    "\n",
    "# --- SCHEDULING LOGIC ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scheduler Started. Waiting for market hours (Mon-Fri, 9:30-16:00)...\")\n",
    "    \n",
    "    # 1. Schedule the job to run every 20 minutes\n",
    "    schedule.every(10).minutes.do(job)\n",
    "\n",
    "    # Define the specific exit time\n",
    "    EXIT_TIME = datetime.time(16, 0)\n",
    "\n",
    "    # 2. Infinite loop to keep the script running\n",
    "    while True:\n",
    "        now = datetime.datetime.now()\n",
    "        \n",
    "        # --- EXIT CHECK ---\n",
    "        # If the current time is past 5:00 PM, break the loop to exit\n",
    "        if now.time() >= EXIT_TIME:\n",
    "            print(f\"Current time is {now.strftime('%H:%M')}. Past 5:00 PM. Exiting program.\")\n",
    "            break\n",
    "\n",
    "        # Check if today is a weekday (0=Mon, 4=Fri)\n",
    "        if now.weekday() < 5:\n",
    "            schedule.run_pending()\n",
    "        else:\n",
    "            # Optional: Pass on weekends\n",
    "            pass\n",
    "            \n",
    "        # Check every 60 seconds (better accuracy than 500s)\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ad92d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date ticker       open       high        low       close  \\\n",
      "0  2001-01-02 00:00:00    SPY  132.00000  132.15625  127.56250  128.812500   \n",
      "1  2001-01-03 00:00:00    SPY  128.31250  136.00000  127.65625  135.000000   \n",
      "2  2001-01-04 00:00:00    SPY  134.93750  135.46875  133.00000  133.546875   \n",
      "3  2001-01-05 00:00:00    SPY  133.46875  133.62500  129.18750  129.187500   \n",
      "4  2001-01-08 00:00:00    SPY  129.87500  130.18750  127.68750  130.187500   \n",
      "\n",
      "   adj_close    volume  \n",
      "0  81.997665   8737500  \n",
      "1  85.936356  19431600  \n",
      "2  85.011391   9219000  \n",
      "3  82.236374  12911400  \n",
      "4  82.872902   6625300  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"sp500_data.db\")\n",
    "\n",
    "# Example: Get all Apple data\n",
    "df = pd.read_sql(\"SELECT * FROM stock_history WHERE ticker = 'SPY'\", conn)\n",
    "print(df.head())\n",
    "\n",
    "# Example: Get all closing prices for a specific date\n",
    "df_date = pd.read_sql(\"SELECT ticker, close FROM stock_history WHERE date = '2023-01-04 00:00:00'\", conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f03291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-02 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.156250</td>\n",
       "      <td>127.562500</td>\n",
       "      <td>128.812500</td>\n",
       "      <td>81.997665</td>\n",
       "      <td>8737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-03 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>128.312500</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>127.656250</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>85.936356</td>\n",
       "      <td>19431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-04 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>134.937500</td>\n",
       "      <td>135.468750</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.546875</td>\n",
       "      <td>85.011391</td>\n",
       "      <td>9219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-05 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>133.468750</td>\n",
       "      <td>133.625000</td>\n",
       "      <td>129.187500</td>\n",
       "      <td>129.187500</td>\n",
       "      <td>82.236374</td>\n",
       "      <td>12911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-08 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>129.875000</td>\n",
       "      <td>130.187500</td>\n",
       "      <td>127.687500</td>\n",
       "      <td>130.187500</td>\n",
       "      <td>82.872902</td>\n",
       "      <td>6625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12555</th>\n",
       "      <td>2025-12-15 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>685.739990</td>\n",
       "      <td>685.760010</td>\n",
       "      <td>679.250000</td>\n",
       "      <td>680.729980</td>\n",
       "      <td>678.724426</td>\n",
       "      <td>90811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>2025-12-16 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>679.229980</td>\n",
       "      <td>681.080017</td>\n",
       "      <td>674.979980</td>\n",
       "      <td>678.869995</td>\n",
       "      <td>676.869934</td>\n",
       "      <td>122030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12557</th>\n",
       "      <td>2025-12-17 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>679.890015</td>\n",
       "      <td>680.440002</td>\n",
       "      <td>671.200012</td>\n",
       "      <td>671.400024</td>\n",
       "      <td>669.421936</td>\n",
       "      <td>110625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12558</th>\n",
       "      <td>2025-12-18 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>677.599976</td>\n",
       "      <td>680.739990</td>\n",
       "      <td>674.900024</td>\n",
       "      <td>676.469971</td>\n",
       "      <td>674.476929</td>\n",
       "      <td>108650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12559</th>\n",
       "      <td>2025-12-19 00:00:00</td>\n",
       "      <td>SPY</td>\n",
       "      <td>676.590027</td>\n",
       "      <td>681.090027</td>\n",
       "      <td>676.469971</td>\n",
       "      <td>680.590027</td>\n",
       "      <td>680.590027</td>\n",
       "      <td>103506100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12560 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date ticker        open        high         low  \\\n",
       "0      2001-01-02 00:00:00    SPY  132.000000  132.156250  127.562500   \n",
       "1      2001-01-03 00:00:00    SPY  128.312500  136.000000  127.656250   \n",
       "2      2001-01-04 00:00:00    SPY  134.937500  135.468750  133.000000   \n",
       "3      2001-01-05 00:00:00    SPY  133.468750  133.625000  129.187500   \n",
       "4      2001-01-08 00:00:00    SPY  129.875000  130.187500  127.687500   \n",
       "...                    ...    ...         ...         ...         ...   \n",
       "12555  2025-12-15 00:00:00    SPY  685.739990  685.760010  679.250000   \n",
       "12556  2025-12-16 00:00:00    SPY  679.229980  681.080017  674.979980   \n",
       "12557  2025-12-17 00:00:00    SPY  679.890015  680.440002  671.200012   \n",
       "12558  2025-12-18 00:00:00    SPY  677.599976  680.739990  674.900024   \n",
       "12559  2025-12-19 00:00:00    SPY  676.590027  681.090027  676.469971   \n",
       "\n",
       "            close   adj_close     volume  \n",
       "0      128.812500   81.997665    8737500  \n",
       "1      135.000000   85.936356   19431600  \n",
       "2      133.546875   85.011391    9219000  \n",
       "3      129.187500   82.236374   12911400  \n",
       "4      130.187500   82.872902    6625300  \n",
       "...           ...         ...        ...  \n",
       "12555  680.729980  678.724426   90811000  \n",
       "12556  678.869995  676.869934  122030600  \n",
       "12557  671.400024  669.421936  110625200  \n",
       "12558  676.469971  674.476929  108650100  \n",
       "12559  680.590027  680.590027  103506100  \n",
       "\n",
       "[12560 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32754b",
   "metadata": {},
   "source": [
    "\n",
    "## Get the yfiance fundamental data ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b326d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper started. Press Ctrl+C to stop.\n",
      "Error fetching S&P list: HTTP Error 403: Forbidden\n",
      "Could not retrieve tickers. Retrying later.\n",
      "Sleeping for 24.0 hours...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_NAME = \"sp500_data.db\"\n",
    "TABLE_NAME = \"stock_fundamentals\"\n",
    "# Time to sleep between individual requests (seconds) to be polite to API\n",
    "REQUEST_DELAY = 1.5 \n",
    "# Time to sleep after finishing the full S&P 500 list (seconds) - 24 Hours\n",
    "CYCLE_DELAY = 86400 \n",
    "\n",
    "# The mapping of Yahoo API Keys -> Your Database Column Names\n",
    "FUNDAMENTAL_MAPPING = {\n",
    "    # Price & Volume\n",
    "    'currentPrice': 'current_price',\n",
    "    'totalCash': 'total_cash',\n",
    "    'totalCashPerShare': 'total_cash_per_share',\n",
    "    'ebitda': 'ebitda',\n",
    "    'totalDebt': 'total_debt',\n",
    "    'quickRatio': 'quick_ratio',\n",
    "    'currentRatio': 'current_ratio',\n",
    "    'totalRevenue': 'total_revenue',\n",
    "    'debtToEquity': 'debt_to_equity',\n",
    "    'revenuePerShare': 'revenue_per_share',\n",
    "    'returnOnAssets': 'return_on_assets',\n",
    "    'returnOnEquity': 'return_on_equity',\n",
    "    'grossProfits': 'gross_profits',\n",
    "    'freeCashflow': 'free_cashflow',\n",
    "    'operatingCashflow': 'operating_cashflow',\n",
    "    'earningsGrowth': 'earnings_growth',\n",
    "    'revenueGrowth': 'revenue_growth',\n",
    "    'grossMargins': 'gross_margins',\n",
    "    'ebitdaMargins': 'ebitda_margins',\n",
    "    'operatingMargins': 'operating_margins',\n",
    "    'dividendRate': 'dividend_rate',\n",
    "    'dividendYield': 'dividend_yield',\n",
    "    'trailingPE': 'trail_PE',\n",
    "    'forwardPE': 'forward_PE',\n",
    "    'trailingEps': 'trail_Eps',\n",
    "    'forwardEps': 'forward_Eps',\n",
    "    'bookValue': 'book_value',\n",
    "    'priceToBook': 'price_2_book',\n",
    "    'enterpriseValue': 'company_value',\n",
    "    'sharesOutstanding': 'outstanding_share',\n",
    "    'heldPercentInsiders': 'insider_held_percent',\n",
    "    'heldPercentInstitutions': 'institution_held_percent',\n",
    "    'shortRatio': 'short_ratio',\n",
    "    'beta': 'beta'\n",
    "}\n",
    "\n",
    "TICKERS = [ \"SPY\" ,\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\" \\\n",
    "           ,\"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\" \\\n",
    "            ,\"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\" \\\n",
    "            ,\"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" ] \n",
    "\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Fetches current S&P 500 tickers from Wikipedia.\"\"\"\n",
    "    try:\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        tables = pd.read_html(url)\n",
    "        df = tables[0]\n",
    "        tickers = df['Symbol'].str.replace('.', '-', regex=False).tolist()\n",
    "        return tickers\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching S&P list: {e}\")\n",
    "        return []\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Creates the table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Dynamic schema creation based on our mapping\n",
    "    # All columns will be REAL (numbers) or TEXT based on SQLite flexibility\n",
    "    cols = \", \".join([f\"{col} REAL\" for col in FUNDAMENTAL_MAPPING.values()])\n",
    "    \n",
    "    create_stmt = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        ticker TEXT,\n",
    "        scraped_date TEXT,\n",
    "        {cols},\n",
    "        PRIMARY KEY (ticker, scraped_date)\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_stmt)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def already_scraped(ticker, date_str):\n",
    "    \"\"\"Checks if we already have data for this ticker on this date.\"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT 1 FROM {TABLE_NAME} WHERE ticker = ? AND scraped_date = ?\", (ticker, date_str))\n",
    "    exists = cursor.fetchone() is not None\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "def run_daily_cycle():\n",
    "    \"\"\"Runs one full pass of the S&P 500.\"\"\"\n",
    "    tickers = get_sp500_tickers()\n",
    "    if not tickers:\n",
    "        print(\"Could not retrieve tickers. Retrying later.\")\n",
    "        return\n",
    "\n",
    "    today_str = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"--- Starting scrape cycle for {today_str} ({len(tickers)} tickers) ---\")\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # 1. Check for duplicates\n",
    "        if already_scraped(ticker, today_str):\n",
    "            print(f\"[{ticker}] Already scraped for today. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 2. Fetch Data\n",
    "            # Using Ticker(ticker) is faster than yf.download for fundamentals\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            \n",
    "            # 3. Build Row\n",
    "            row_data = {\n",
    "                'ticker': ticker,\n",
    "                'scraped_date': today_str\n",
    "            }\n",
    "            \n",
    "            # Map keys and handle NULLs\n",
    "            for api_key, db_col in FUNDAMENTAL_MAPPING.items():\n",
    "                # .get(key, None) ensures that if the key is missing, \n",
    "                # Python uses None, which becomes NULL in SQLite.\n",
    "                val = info.get(api_key, None)\n",
    "                row_data[db_col] = val\n",
    "\n",
    "            # 4. Save to DB\n",
    "            df = pd.DataFrame([row_data])\n",
    "            conn = sqlite3.connect(DB_NAME)\n",
    "            df.to_sql(TABLE_NAME, conn, if_exists=\"append\", index=False)\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"[{ticker}] Success.\")\n",
    "            \n",
    "            # Sleep to be polite\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{ticker}] Error: {e}\")\n",
    "            # If there is an error, we continue to the next ticker\n",
    "            continue\n",
    "\n",
    "    print(f\"--- Cycle for {today_str} complete. ---\")\n",
    "\n",
    "def main():\n",
    "    # Setup database once on start\n",
    "    init_db()\n",
    "    \n",
    "    print(\"Scraper started. Press Ctrl+C to stop.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            run_daily_cycle()\n",
    "            \n",
    "            print(f\"Sleeping for {CYCLE_DELAY/3600} hours...\")\n",
    "            time.sleep(CYCLE_DELAY)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping script manually.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Critical Error in main loop: {e}\")\n",
    "            print(\"Retrying in 1 minute...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "class StockScraper:\n",
    "    def __init__(self, ticker, db_name, db_folder):\n",
    "        \"\"\"\n",
    "        Initializes the scraper and sets up the database connection.\n",
    "        \"\"\"\n",
    "        # 1. Setup Database Path\n",
    "        if not os.path.exists(db_folder):\n",
    "            os.makedirs(db_folder)\n",
    "        \n",
    "        self.db_path = os.path.join(db_folder, db_name)\n",
    "        self.table_name = ticker\n",
    "        self.request_delay = 1.5\n",
    "        \n",
    "        # 2. Define Mapping (API Key -> DB Column)\n",
    "        self.mapping = {\n",
    "            'last_price': 'last',\n",
    "            'bid': 'bid',\n",
    "            'ask': 'ask',\n",
    "            'bidSize': 'bid_size',\n",
    "            'askSize': 'ask_size',\n",
    "            'currentPrice': 'current_price',\n",
    "            'targetHighPrice': 'target_high_price',\n",
    "            'targetLowPrice': 'target_low_price',\n",
    "            'targetMeanPrice': 'target_mean_price',\n",
    "            'targetMedianPrice': 'target_median_price',\n",
    "            'recommendationKey': 'recommendation_key', # This is TEXT (e.g., \"buy\")\n",
    "            'numberOfAnalystOpinions': 'number_of_analyst_opinions',\n",
    "            'lastDividendValue': 'last_dividend_value',\n",
    "            'lastDividendDate': 'last_dividend_date',\n",
    "            'totalCash': 'total_cash',\n",
    "            'totalCashPerShare': 'total_cash_per_share',\n",
    "            'ebitda': 'ebitda',\n",
    "            'totalDebt': 'total_debt',\n",
    "            'quickRatio': 'quick_ratio',\n",
    "            'currentRatio': 'current_ratio',\n",
    "            'totalRevenue': 'total_revenue',\n",
    "            'debtToEquity': 'debt_to_equity',\n",
    "            'revenuePerShare': 'revenue_per_share',\n",
    "            'returnOnAssets': 'return_on_assets',\n",
    "            'returnOnEquity': 'return_on_equity',\n",
    "            'grossProfits': 'gross_profits',\n",
    "            'freeCashflow': 'free_cashflow',\n",
    "            'operatingCashflow': 'operating_cashflow',\n",
    "            'earningsGrowth': 'earnings_growth',\n",
    "            'revenueGrowth': 'revenue_growth',\n",
    "            'grossMargins': 'gross_margins',\n",
    "            'ebitdaMargins': 'ebitda_margins',\n",
    "            'operatingMargins': 'operating_margins',\n",
    "            'regularMarketChange': 'regular_market_change',\n",
    "            'regularMarketChangePercent': 'regular_market_change_percent',\n",
    "            'regularMarketPrice': 'regular_market_price',\n",
    "            'postMarketChangePercent': 'post_market_change_percent',\n",
    "            'postMarketPrice': 'post_market_price',\n",
    "            'postMarketChange': 'post_market_change',\n",
    "            'averageDailyVolume3Month': 'average_daily_volume_3_month',\n",
    "            'open': 'open_price',\n",
    "            'dayLow': 'day_low',\n",
    "            'dayHigh': 'day_high',\n",
    "            'regularMarketPreviousClose': 'regular_market_previous_close',\n",
    "            'regularMarketOpen': 'regular_market_open',\n",
    "            'regularMarketDayLow': 'regular_market_day_low',\n",
    "            'regularMarketDayHigh': 'regular_market_day_high',\n",
    "            'dividendRate': 'dividend_rate',\n",
    "            'dividendYield': 'dividend_yield',\n",
    "            'trailingPE': 'trail_PE',\n",
    "            'forwardPE': 'forward_PE',\n",
    "            'trailingEps': 'trail_Eps',\n",
    "            'forwardEps': 'forward_Eps',\n",
    "            'previous_close': 'prev_close',\n",
    "            'volume': 'volume',\n",
    "            'regularMarketVolume': 'reg_market_volume',\n",
    "            'averageVolume': 'avg_volume',\n",
    "            'averageVolume10days': 'ave_10d_volume',\n",
    "            'currency': 'currency', # This is TEXT (e.g., \"USD\")\n",
    "            'exchange': 'exchange', # This is TEXT (e.g., \"NMS\")\n",
    "            'profitMargins': 'profit_margin',\n",
    "            'floatShares': 'float_shares',\n",
    "            'enterpriseValue': 'company_value',\n",
    "            'sharesOutstanding': 'outstanding_share',\n",
    "            'sharesShort': 'share_short',\n",
    "            'sharesPercentSharesOut': 'share_percent_shares_out',\n",
    "            'heldPercentInsiders': 'insider_held_percent',\n",
    "            'heldPercentInstitutions': 'institution_held_percent',\n",
    "            'shortRatio': 'short_ratio',\n",
    "            'shortPercentOfFloat': 'short_percent_of_float',\n",
    "            'impliedSharesOutstanding': 'implied_shares_outstanding',\n",
    "            'bookValue': 'book_value',\n",
    "            'priceToBook': 'price_2_book'\n",
    "        }\n",
    "        \n",
    "        # 3. Initialize DB Schema immediately\n",
    "        self._init_db()\n",
    "\n",
    "    def _get_conn(self):\n",
    "        \"\"\"Helper to get a database connection.\"\"\"\n",
    "        return sqlite3.connect(self.db_path)\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"Creates the table if it doesn't exist.\"\"\"\n",
    "        conn = self._get_conn()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # --- CHANGE 1: DYNAMIC TYPING ---\n",
    "        # Instead of f\"{col} REAL\", we just use f\"{col}\".\n",
    "        # This tells SQLite to accept ANY data type (Text, Integer, Float, Null)\n",
    "        # which is perfect for mixed data (prices, currency codes, ratings).\n",
    "        cols = \", \".join([f\"{col}\" for col in self.mapping.values()])\n",
    "        \n",
    "        # --- CHANGE 2: ADDED TIMESTAMP COLUMN ---\n",
    "        create_stmt = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
    "            ticker TEXT,\n",
    "            scraped_date TEXT,\n",
    "            timestamp TEXT,\n",
    "            {cols},\n",
    "            PRIMARY KEY (ticker, scraped_date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_stmt)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def _is_already_scraped(self, ticker, date_str):\n",
    "        \"\"\"Checks if data exists for this ticker/date combo.\"\"\"\n",
    "        conn = self._get_conn()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            f\"SELECT 1 FROM {self.table_name} WHERE ticker = ? AND scraped_date = ?\", \n",
    "            (ticker, date_str)\n",
    "        )\n",
    "        exists = cursor.fetchone() is not None\n",
    "        conn.close()\n",
    "        return exists\n",
    "\n",
    "    def _fetch_sp500_list(self):\n",
    "        \"\"\"Internal helper to get S&P 500 tickers from Wiki.\"\"\"\n",
    "        try:\n",
    "            url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "            tables = pd.read_html(url)\n",
    "            # Replace dots with dashes (BRK.B -> BRK-B)\n",
    "            return tables[0]['Symbol'].str.replace('.', '-', regex=False).tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching S&P list: {e}\")\n",
    "            return []\n",
    "\n",
    "    # ==========================================\n",
    "    # Public Methods\n",
    "    # ==========================================\n",
    "\n",
    "    def scrape_single(self, ticker, force_update=False):\n",
    "        \"\"\"\n",
    "        Scrapes a single ticker. \n",
    "        Returns True if successful, False if failed or skipped.\n",
    "        \"\"\"\n",
    "        # Get current date and exact timestamp\n",
    "        now = datetime.datetime.now()\n",
    "        today_str = now.strftime(\"%Y-%m-%d\")\n",
    "        timestamp_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if not force_update and self._is_already_scraped(ticker, today_str):\n",
    "            print(f\"[{ticker}] Already scraped for {today_str}. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            \n",
    "            # --- CHANGE 3: SAVE TIMESTAMP ---\n",
    "            row_data = {\n",
    "                'ticker': ticker, \n",
    "                'scraped_date': today_str,\n",
    "                'timestamp': timestamp_str\n",
    "            }\n",
    "            \n",
    "            # Map data\n",
    "            for api_key, db_col in self.mapping.items():\n",
    "                row_data[db_col] = info.get(api_key, None)\n",
    "\n",
    "            # Save\n",
    "            conn = self._get_conn()\n",
    "            df = pd.DataFrame([row_data])\n",
    "            df.to_sql(self.table_name, conn, if_exists=\"append\", index=False)\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"[{ticker}] Successfully saved at {timestamp_str}.\")\n",
    "            time.sleep(self.request_delay) # Be polite\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{ticker}] Error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def scrape_custom_list(self, ticker_list):\n",
    "        \"\"\"Scrapes a specific list of tickers (e.g., your custom portfolio).\"\"\"\n",
    "        print(f\"--- Starting Custom Scrape ({len(ticker_list)} tickers) ---\")\n",
    "        for ticker in ticker_list:\n",
    "            self.scrape_single(ticker)\n",
    "        print(\"--- Custom Scrape Complete ---\")\n",
    "\n",
    "    def scrape_sp500(self):\n",
    "        \"\"\"Scrapes the entire S&P 500 index.\"\"\"\n",
    "        tickers = self._fetch_sp500_list()\n",
    "        print(f\"--- Starting S&P 500 Scrape ({len(tickers)} tickers) ---\")\n",
    "        for ticker in tickers:\n",
    "            self.scrape_single(ticker)\n",
    "        print(\"--- S&P 500 Scrape Complete ---\")\n",
    "\n",
    "# ==========================================\n",
    "# Execution Example\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Initialize the Scraper\n",
    "    scraper = StockScraper(db_name=\"my_portfolio.db\", db_folder=\"data\")\n",
    "\n",
    "    # 2. Your Custom List\n",
    "    MY_TICKERS = [ \n",
    "        \"SPY\", \"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\",\n",
    "        \"GOOGL\", \"META\", \"TSLA\", \"BRK-B\", \"JPM\",\n",
    "        \"AVGO\", \"INTC\", \"WMT\", \"UNH\", \"V\", \"PYPL\",\n",
    "        \"MA\", \"HD\", \"BAC\", \"COST\", \"DIS\" \n",
    "    ]\n",
    "\n",
    "    # Run the custom list scrape\n",
    "    scraper.scrape_custom_list(MY_TICKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79d346",
   "metadata": {},
   "source": [
    "## This is to check for the url connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "try:\n",
    "    tables = pd.read_html(url)\n",
    "    df = tables[0]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad48bb03",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tables = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\html.py:983\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m p = parser(\n\u001b[32m    973\u001b[39m     io,\n\u001b[32m    974\u001b[39m     compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m     storage_options,\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     tables = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[32m    985\u001b[39m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[33m\"\u001b[39m\u001b[33mseekable\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io.seekable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\html.py:249\u001b[39m, in \u001b[36m_HtmlFrameParser.parse_tables\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m \u001b[33;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     tables = \u001b[38;5;28mself\u001b[39m._parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.match, \u001b[38;5;28mself\u001b[39m.attrs)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\html.py:806\u001b[39m, in \u001b[36m_LxmlFrameParser._build_doc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    804\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    805\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(r, \u001b[33m\"\u001b[39m\u001b[33mtext_content\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\html.py:785\u001b[39m, in \u001b[36m_LxmlFrameParser._build_doc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m.io):\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    788\u001b[39m             r = parse(f.handle, parser=parser)\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\site-packages\\pandas\\io\\common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    558\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\stock\\Lib\\urllib\\request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "tables = pd.read_html(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
